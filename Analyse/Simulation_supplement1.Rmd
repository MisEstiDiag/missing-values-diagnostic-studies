---
title: 'Additional methods to handle missing values in a continuous index test in a diagnostic study – supplemental simulation scenarios (Additional file 1)'
author: 
 - Katharina Stahlmann, University Medical Center Hamburg-Eppendorf<br>  Institute of Medical Biometry and Epidemiology, k.stahlmann@uke.de,
 - Bastiaan Kellerhuis, Julius Center for Health Sciences and Primary Care, University Medical Center Utrecht, Utrecht University, Utrecht, The Netherlands
 - Johannes B. Reitsma, Julius Center for Health Sciences and Primary Care, University Medical Center Utrecht, Utrecht University, Utrecht, The Netherlands
 - Nandini Dendukuri, Department of Medicine, McGill University, Montreal, Canada
 - Antonia Zapf, University Medical Center Hamburg-Eppendorf<br>  Institute of Medical Biometry and Epidemiology
date: "`r Sys.Date()`"
output:
  html_document:
    depth: 6
    fig_caption: no
    number_sections: yes
    theme: united
    toc: yes
    toc_float: yes
    code_folding: hide
  word_document:
    toc: yes
editor_options:
  chunk_output_type: console
always_allow_html: yes
---

```{r setup}

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
                       
# set global chunk options
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.width = 12,
                       fig.asp = 0.8 ,
                       out.width = "120%")

# this options for word export
#knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

options(stringsAsFactors = F)
# always show NA is there is any in the table function
table = function (..., useNA = 'ifany') base::table(..., useNA = useNA)
```

# Settings

## Packages

```{r packages}
library(ggplot2)
library(writexl)
library(ggpubr)
library(RColorBrewer)
library(dplyr)
library(arsenal)
library(rsimsum)
library(gt)
```

## load functions

```{r}

mycontrols = tableby.control(numeric.stats=c("Nmiss","mean", "sd", "medianq1q3", "range"),
                             cat.stats=c("Nmiss", "countpct"), 
                             stats.labels=list(Nmiss='Missing values', medianq1q3='Median (Q1, Q3)'),
                             test = F)

colors <- brewer.pal(7, "Dark2")
```

# Simulation 

## Set up simulation parameters

```{r}
nsim <- 100

#define simulation scenarios
grid = expand.grid(
  sim = 1:nsim 
  , N = c(500)
  , p = c(0.3)
  , AUC_0 = c(0.85)
  , r = c(0.5)
  , pm = c(0.1, 0.3, 0.5)
  , mech = c("MCAR", "MAR", "MNAR")
)

# methods to be compared
methods <- c("CCA", "CONV", "mi_", "mice_n", "mice_p", "IPL.LG", "IPL.NP")

Abbreviation = names(grid)
setup <- data.frame(
  Abbreviation = names(grid),
  Parameter = c("Number of simulations", "Sample size", "prevalence of the target condition", "True AUC", "Correlation between index test and covariates", "proportion of missing values", "Missingness Mechanism"),
  Values = c(
    nsim,
    sapply(Abbreviation[2:length(Abbreviation)], function(x) (paste(unique(grid[,x]), collapse = ',')))
  )
)  
setup %>%
  gt() %>% 
  tab_style(style = cell_text(weight = "bold"), locations = cells_column_labels(columns=c("Abbreviation", "Parameter", "Values"))) %>%
  tab_header(
    title = "Table 1. Overview of simulation parameter"
  )

```

## Description of methods

Table 2. Overview of methods

| Methods     |                                                                                                           |
|:------------|:----------------------------------------------------------------------------------------------------------|
| CCA       	| Complete case analysis                                                                                    |
| CONV       	| Convolution-based approach (Bianco et al. 2023)                                                           |
| mi          | Multiple Imputation using chained equations with predictive mean matching (PMM) (Su et al. 2022)          |
| mice (pmm)  | Multiple Imputation using chained equations with predictive mean matching (PMM) (van Buuren et al. 2011)  |
| mice (norm) | Multiple Imputation using chained equations with Bayesian linear regression (van Buuren et al. 2011)      |
| IPL LG      | Hybrid Imputation using a logistic model (Cheng and Tang 2020)                                            |
| IPL NP      | Hybrid Imputation non-parametric (Cheng and Tang 2020)                                                    |

### Mice 
Multiple imputations by chained equations is a standard multiple imputation approach that employs fully conditional specification via Markov Chain Monte Carlo methods. It can be accessed through the mice package in R (van Buuren and Groothuis-Oudshoorn 2011). For each variable with missing data, conditional densities are specified. Multivariate imputation is performed variable-by-variable and iterating over the conditional densities. We selected predictive mean matching as imputation method since this is the default method for numeric variables in mice and performs well overall (van Buuren and Groothuis-Oudshoorn 2011). As Bayesian linear regression may be more efficient for normally distributed data (van Buuren 2018), we include mice using this imputation method (called norm) in addition to mice with PMM in this supplemental simulation scenarios. We set the number of iterations to 10 and generated 20 imputation datasets for our simulation as Schafer and Graham (2002) deemed 20 imputations to remove most noise in the estimates. All variables (index test, the three covariates and the reference standard) without interaction terms were included in the imputation model. After imputation, estimators for the AUC and sensitivities for different FPR values are pooled by Rubin’s rules (Rubin 1987).

### Mi
The R package mi was published by Su et al. (2011) and employs fully conditional specification as well. To be consistent, predictive mean matching was selected as imputation method, too. In congruence with mice, we selected 10 iterations, generated 20 imputation datasets and used all variables but no interaction terms in the imputation model. The results are only reproducible if the parallel argument is set to FALSE in the function, and this method could not be executed on the high-performance cluster. Thus, we decided to include this method only in this small set of supplemental scenarios. 

### CONV 
Bianco et al. (2023) calculated a convolution-based estimator assuming that covariates (in this simulation study x2, x3, x4) are related to the index test values through a regression model. This regression function may be parametric, non-parametric, or semi-parametric. Bianco et al. (2023) assumed a parametric linear regression model. The convolution-based estimator is defined as the convolution of the regression function distribution and the errors and is a weighted empirical distribution. The sensitivity values for different FPR values are calculated following this approach, and subsequently, the AUC is calculated based on the sensitivity values. Upon request, the authors provided this method as R code. As this method takes very long to run, we excluded it from the main simulation study but examined it within this reduced set of supplemental scenarios. 

### IPL-LG and IPL-NP 
Cheng and Tang (2020) proposed two smoothed empirical likelihood methods combining multiple imputation and inverse probability-weighted imputation (i.e. hybrid imputation based on estimating equations). Auxiliary variables can be incorporated into the propensity score function. They proposed a parametric propensity score using a logistic regression (IPL-LG) and a nonparametric propensity score function (IPL-NP). Missing values are assumed to be MAR. Both methods are available as R code in their supplemental material and are programmed to estimate sensitivity values and their CI for defined FPR values. We further extended their code to estimate the AUC and its CI as well and set the same parameter options as Cheng and Tang (2020): the number of imputation = 20. These both methods are not included in the main simulation study but only in a reduced simulation study with 9 scenarios and 100 repetitions. We decided to exclude them from the main simulation, as it is not clear to us why and how they rescale the index test and covariate values before running their method. In addition, both methods take much time. 

### CCA 
We compared the above explained methods with the standard complete case analysis (CCA) which excludes all subjects with missing values in the index test (or reference standard) from the calculations. For the application of the CCA, we used the ci.auc function of the pROC package (Robin et al. 2011).

## Conduct the simulation

We cannot conduct the simulation programs within this Rmarkdown file, as they were executed on a high performance cluster. However, we provide the R programs and the results of the simulations to enable reproduction. The seed for the parallel loop was set to 5273 and to 4730xi for data generation.   

Load the simulation results

```{r}
# Note: Owing to very long running times of some methods, we ran the simulation for each method separately. Thus we need to combine the data for each method for both parts in the following

data <- c("results_conv", "results_mi", "results_mice_n", "results_mice_p", "results_ipl")
list_dta <- list()

for (i in 1:length(data)) {
  
  load(file = paste0("./Analyse/hpc/Simulation_data/", data[i], ".Rdata"))
  res$id <- paste(res$sim, res$scenario, sep = "_")
  list_dta[[i]] <- res
  assign(paste0(data[i]), list_dta[[i]])
}

# merge results by id
res_dta <- merge(results_conv, results_mi[,c(10:ncol(results_mi))], by="id")
res_dta <- merge(res_dta, results_mice_n[,c(10:ncol(results_mice_n))], by="id")
res_dta <- merge(res_dta, results_mice_p[,c(10:ncol(results_mice_p))], by="id")
res_dta <- merge(res_dta, results_ipl[,c(10:ncol(results_ipl))], by="id")

res <- res_dta %>%
  arrange(scenario, sim) %>%
  select(-id) %>%
  rename(r = korr)

```


# Calculate Performance parameter

The following performance parameter will be calculated: number of missing values, bias, root mean squared error, empirical standard deviation, coverage, bias-eliminated coverage and power as well as the respective monte carlo standard errors for each performance parameter. 

```{r}
source("Analyse/Simulation_performance3.R")
# input: res (file with simulation results as dataframe)
# out: raw_res (res + step 1 of performance calculation -> 1 row for each simulation run), scenario_res (dataframe with performance parameters, -> 1 row for each scenario aggregated over the runs)
```

# Results

## Table of performance parameter 

```{r, results='asis'}
# rearrange the scenario_res table columns, so that performance estimate and its MC standard error are located side by side
vars=list()
for (i in 1:length(methods)){
  vars_i <- grep(methods[i], names(scenario_res), value = T)
  vars[[i]] <- vars_i
}
vars_order <- unlist(vars)
dat_table1 <- scenario_res[,c(8,1:6)]
dat_table2 <- scenario_res[,vars_order]
dat_table <- cbind(dat_table1, dat_table2)
rownames(dat_table) <- NULL

# label selected variables in dat_table
dat_table <- dat_table %>%
  sjlabelled::var_labels(
    scenario = "Scenario",
    N = "Sample size",
    p = "Prevalence of target condition",
    AUC_0 = "True AUC",
    r = "Correlation",
    pm = "Proportion of missing values",
    mech = "Missingness mechanism",
    av.time.AUC.CCA = "Average running time for CCA",
    av.time.AUC.CONV = "Average running time for CONV",
    av.time.AUC.mi_ = "Average running time for mi",
    av.time.AUC.mice_n = "Average running time for mice (norm)",
    av.time.AUC.mice_p = "Average running time for mice (pmm)",
    av.time.AUC.IPL.LG = "Average running time for IPL LG",
    av.time.AUC.IPL.NP = "Average running time for IPL NP"
  )


# show table for bias
bias <- grep("bias", names(dat_table), value = FALSE)
knitr::kable(dat_table[,c(1:7,bias)], "simple", 
             col.names = gsub("[.]", " ", names(dat_table[,c(1:7,bias)])), 
             caption = "Table 3. Bias and its Monte Carlo Standard Error for each method",
             digits = 4, format.args = list(scientific = FALSE))

# show table for RMSE 
mse <- grep("MSE", names(dat_table), value = FALSE)
knitr::kable(dat_table[,c(1:7,mse)], "simple", 
             col.names = gsub("[.]", " ", names(dat_table[,c(1:7,mse)])),
             caption = "Table 4. Rot mean squared error and its Monte Carlo Standard Error for each method",
             digits = 4, format.args = list(scientific = FALSE))

# show table for empSE
empse <- grep("empSE", names(dat_table), value = FALSE)
knitr::kable(dat_table[,c(1:7,empse)], "simple", 
             col.names = gsub("[.]", " ", names(dat_table[,c(1:7,empse)])),
             caption = "Table 5. Empirical Standard Error and its Monte Carlo Standard Error for each method",
             digits = 4, format.args = list(scientific = FALSE))

# show table for coverage
cov <- grep("cov", names(dat_table), value = FALSE)
knitr::kable(dat_table[,c(1:7,cov)], "simple", 
             col.names = gsub("[.]", " ", names(dat_table[,c(1:7,cov)])),
             caption = "Table 6. Coverage and its Monte Carlo Standard Error for each method",
             digits = 4, format.args = list(scientific = FALSE))

# show table for power
power <- grep("power", names(dat_table), value = FALSE)
knitr::kable(dat_table[,c(1:7,power)], "simple", 
             col.names = gsub("[.]", " ", names(dat_table[,c(1:7,power)])),
             caption = "Table 7. Power and its Monte Carlo Standard Error for each method",
             digits = 4, format.args = list(scientific = FALSE))


# save results in excel file
write_xlsx(scenario_res, path = "./Analyse/Ergebnisse/res_suppl.xlsx")
```

Table 8. Overview of average running time (in seconds) summarized across all scenarios
```{r, results='asis'}

# table with running time summarized across all scenarios 
time <- grep("time", names(dat_table), value = TRUE)
summary(tableby( ~ ., data = dat_table[,c(time)], control = mycontrols), pfootnote = T) 

```

There are no missing values in the estimation of the AUC

```{r, include=FALSE, results='asis', eval=FALSE}
# only for scenarios with missing values
miss <- grep("Missing", names(dat_table), value = TRUE)
dat_table_miss <- dat_table %>%
  select(c(1:8, all_of(miss))) %>%
  mutate(
    rowmiss = select(., starts_with("Missing")) %>% rowSums()
  ) %>%
  filter(rowmiss>0) %>%
  select(-c(rowmiss))
knitr::kable(dat_table_miss, "simple", 
             col.names = gsub("[.]", " ", names(dat_table_miss)), 
             caption = "Table 9. The number of missing values per scenario (sum across all repititions) for each method (only displaying scenarios with missing values",
             digits = 4, format.args = list(scientific = FALSE))
    

```


## Graphical display of performance results

### Bias 

```{r}

# reshape from wide to long (only one column for bias and estimated AUC, respectively)
performparam <- list(auc_vars,names1)
performnames <- c("AUC","Bias")
dat_fig <- raw_res[,c("scenario","N","p","AUC_0","r","pm","mech",auc_vars,names1)]
dat_fig$id <- seq_along(1:nrow(dat_fig))
dat_long <- reshape(dat_fig, varying=performparam, v.names = performnames, times = methods, 
                    idvar = "id", direction = "long")
dat_long$scenario <- as.factor(dat_long$scenario)
colnames(dat_long)[colnames(dat_long) == "time"] <- "Method" # rename method variable

```


```{r}

plot_bias <- ggplot(dat_long, aes(x = Method, y = Bias)) +
                          geom_violin(fill = colors[1], trim=FALSE) +
                          stat_summary(fun=mean, geom="point", size=2, color=colors[6]) +
                          xlab("Method") + ylab("Bias") +
                          geom_hline(yintercept=0) +
                          scale_y_continuous(limits = c(-0.15, 0.1)) +
                          #scale_y_continuous(breaks = c(-0.1,-0.05,0,0.05,0.1), limits = c(-0.125,0.1)) +
                          theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
                          facet_grid(pm ~ mech, labeller = label_both) +
                          ggtitle("Figure 1. Violin plot of bias") +
                          theme(axis.title = element_text(size = 18),
                              axis.text = element_text(size = 16),
                              plot.title = element_text(size = 20),
                              strip.text.x = element_text(size = 14),
                              strip.text.y = element_text(size = 14))
plot_bias
# NOTE: the IPL methods cannot be shown fully as the y-axis is cut at -0.2

```
NOTE: the IPL methods cannot be shown fully as the y-axis is cut at -0.2


```{r}
# reshape summary results for further plots
performparam <- list(names1,names2,names3,names4,names13,names5,names6,names7,names8,names9,names14,names10,names11)
performnames <- c("Bias","RMSE","empirical SE","Coverage","be_coverage",
                  "Power","MCE_bias","MCE_MSE","MCE_empSE","MCE_cov", "MCE_be_cov", "MCE_power", "av_time")
scenario_res$id <- seq_along(1:nrow(scenario_res))
res_long <- reshape(scenario_res, varying=performparam, v.names = performnames, times = methods, 
                    idvar = "id", direction = "long")
colnames(res_long)[colnames(res_long) == "time"] <- "Method" # rename method variable

```

```{r}
## plots for summary performance measures ##

plot_bias2 <- ggplot(res_long, aes(x = pm, y = abs(Bias), group = Method)) +
                      geom_line(aes(color=Method, linetype = Method), linewidth=1) +
                      scale_color_manual(values = colors) +
                      scale_x_continuous(breaks = c(0.1,0.3,0.5)) +
                      xlab("Proportion of missing values") + ylab("Absolute bias") +
                      facet_grid(~ mech, labeller = label_both) + 
                      ggtitle("Figure 2. Mean absolute bias") +
                      theme(axis.title = element_text(size = 18),
                              axis.text = element_text(size = 16),
                              plot.title = element_text(size = 20),
                              strip.text.x = element_text(size = 14),
                              strip.text.y = element_text(size = 14))
plot_bias2

```

### Root mean squared error (RMSE)

```{r}
## plots for summary performance measures ##

plot_rmse <- ggplot(res_long, aes(x = pm, y = RMSE, group = Method)) +
                      geom_line(aes(color=Method, linetype = Method), linewidth=1) +
                      scale_color_manual(values = colors) +
                      scale_x_continuous(breaks = c(0.1,0.3,0.5)) +
                      xlab("Proportion of missing values") + ylab("Root mean squared error") +
                      facet_grid(~ mech, labeller = label_both) + 
                      ggtitle("Figure 3. RMSE") +
                      theme(axis.title = element_text(size = 18),
                              axis.text = element_text(size = 16),
                              plot.title = element_text(size = 20),
                              strip.text.x = element_text(size = 14),
                              strip.text.y = element_text(size = 14))
plot_rmse

```

### Coverage probability

```{r}

# calculate 95% Monte carlo CI for coverage
res_long$MC_cov_ciu <- res_long$Coverage+1.96*res_long$MCE_cov
res_long$MC_cov_cil <- res_long$Coverage-1.96*res_long$MCE_cov

# variable indicating whether bias is "too high" (bias>5%)
res_long$rel_bias <- (res_long$Bias/res_long$AUC_0)*100 # relative bias in %
res_long$bias_cut <- as.factor(if_else(res_long$rel_bias>=5 | res_long$rel_bias<=(-5), "too biased (>=5%)", "acceptable biased"))

plot_conv <- ggplot(res_long, aes(y=Method, x=Coverage, color=bias_cut)) +
                      geom_segment( aes(y=Method, yend=Method, x=0.95, xend=Coverage)) +
                      geom_point(size=2) +
                      geom_vline(xintercept=0.95) +
                      geom_text(aes(MC_cov_ciu, Method, label = ")")) +
                      geom_text(aes(MC_cov_cil, Method, label = "(")) +
                      facet_grid(pm ~ mech, labeller = label_both) + # pm~p
                      scale_color_grey() +
                      ggtitle("Figure 4. Coverage probability") +
                      labs(color="Bias categorized") +
                      theme(axis.title = element_text(size = 18),
                              axis.text = element_text(size = 16),
                              plot.title = element_text(size = 20),
                              strip.text.x = element_text(size = 14),
                              strip.text.y = element_text(size = 14),
                            legend.position = "bottom")
plot_conv

```

### Power 

```{r}

plot_power <- ggplot(res_long, aes(x = pm, y = Power, group = Method)) +
                            geom_line(aes(color=Method, linetype=Method), linewidth=1) +
                            scale_x_continuous(breaks = c(0.1,0.3,0.5)) +
                            scale_color_manual(values = colors) +
                            xlab("Proportion of missing values") + ylab("Power") +
                            facet_grid( ~ mech, labeller = label_both) +
                            ggtitle("Figure 5. Power") +
                            theme(axis.title = element_text(size = 18),
                              axis.text = element_text(size = 16),
                              plot.title = element_text(size = 20),
                              strip.text.x = element_text(size = 14),
                              strip.text.y = element_text(size = 14))
plot_power

```

## Overview of Monte Carlo Errors

Table 9. Summary statistics of Monte Carlo Standard Errors summarized across all scenarios and iterations

```{r, results='asis'}
summary(tableby(Method ~ ., data = res_long[,c("Method", "MCE_bias","MCE_MSE", "MCE_cov", "MCE_power")], control = mycontrols), pfootnote = T) 
```

## Nested loop plots for the "big picture"

Figure 6. Nested loop plot of bias

```{r}
# only pm=0.5
s1 <- simsum(data = dat_long, estvarname = "AUC", ref="CCA", true = "AUC_0", methodvar = "Method", by=c("pm", "mech"))
#summary(s1)

ap <- autoplot(s1, type = "nlp", stats = "bias")
ap + scale_color_manual(values = colors) 

#autoplot(s1, type = "nlp", stats = "mse")
```

# References

Bianco AM, Boente G, González–Manteiga W, Pérez–González A. Estimators for ROC curves with missing biomarkers values and informative covariates. Statistical Methods & Applications. 2023.

Cheng W, Tang N. Smoothed empirical likelihood inference for ROC curve in the presence of missing biomarker values. Biom J. 2020;62(4):1038-59.

Robin X, Turck N, Hainard A, Tiberti N, Lisacek F, Sanchez J, et al. pROC: an open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinformatics. 2011(12):77.

Rubin DB. Multiple Imputation for Nonresponse in Surveys. New York: John Wiley & Sons, Inc.; 1987.

Su Y-S, Gelman A, Hill J, Yajima M. Multiple Imputation with Diagnostics (mi) in R: Opening Windows into the Black Box. Journal of Statistical Software. 2011;45(2):1 - 31.

van Buuren S. Flexible Imputation of Missing Data. 2nd ed. New York: Chapman and Hall/CRC; 2018.

van Buuren S, Groothuis-Oudshoorn K. mice: Multivariate Imputation by Chained Equations in R. Journal of Statistical Software. 2011;45(3):1 - 67.

# Session info

```{r}
sessioninfo::session_info()
```
